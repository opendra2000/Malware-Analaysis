from xgboost import plot_importance
import sklearn.feature_selection
import matplotlib.pyplot as plt
from sklearn import svm
import numpy as np
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics import roc_curve, roc_auc_score
from xgboost import XGBClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import f1_score, accuracy_score, plot_confusion_matrix, auc, confusion_matrix, ConfusionMatrixDisplay, classification_report
import seaborn as sns
import lightgbm as lgb
from sklearn.feature_selection import f_classif
from sklearn.feature_selection import SelectKBest
from matplotlib import pyplot

malData = pd.read_csv("data/finaldata.csv", sep=",", low_memory=True)
print('\n--------------------------------------------Shape of Dataset--------------------------------------------\n')
print(malData.shape)
print('\n Dataset keys')
print(malData.keys())

print('\n--------------------------------------------First five rows: dataset.head()--------------------------------------------\n')
print(malData.head())
print('\n--------------------------------------------DataFrame info- dataset.info()--------------------------------------------\n')
print(malData.info())


print('\n--------------------------------------------Data Cleaning--------------------------------------------\n')
y = malData['label']  # this is the target variable
malData = malData.drop(['hash', 'label'], axis=1)
print(malData.head())
print('Hash and label removed successfully\n')

print('\n--------------------------------------------Data types of features--------------------------------------------\n')
print(malData.dtypes)


print('\n--------------------------------------------Dataset Description for columns: dataset.head()--------------------------------------------\n')
print(malData.describe())


print('\n--------------------------------------------Splitting the dataset--------------------------------------------\n')
X_train, X_test, y_train, y_test = train_test_split(
    malData, y, test_size=0.2, random_state=42)

X_train_backup = X_train
print('X_train.shape = ' + str(X_train.shape))
print('X_test.shape = ' + str(X_test.shape))
print('y_train.shape = ' + str(y_train.shape))
print('y_test.shape = ' + str(y_test.shape))


# using feature selection
select = SelectKBest(score_func=f_classif, k=20)
selected_features = select.fit(X_train, y_train)

# transform train input data
X_train = select.transform(X_train)
# transform test input data
X_test = select.transform(X_test)

featuress1 = list(X_train_backup.columns)

featuress = pd.array(X_train_backup.columns)
filter = select.get_support()
print(featuress[filter])

for i in range(len(select.scores_)):
    print('%s: %f' % (featuress1[i], select.scores_[i]))

# plot for feature importance

# plot the scores
pyplot.bar([featuress1[i] for i in range(len(select.scores_))], select.scores_)
plt.xticks(rotation=30, ha='right')
plt.xlabel('Features')
plt.ylabel('Score')
plt.subplots_adjust(bottom=0.2)  # or whatever

pyplot.show()

print('\n\n--------------------------------------------------------Classifiers--------------------------------------------------------\n')

print('\n----------------------------------------------------------------------------------------------------------------------\n')
#
#
#
#
#

print('\n-------------------------------- Logistic Regression=== standardized ----------------------------------\n')

clf = LogisticRegression(random_state=0, C=1, max_iter=100,
                         penalty='l2', class_weight={1: 0.5, 0: 0.5})
# Create the scaling method
ss = StandardScaler()
# Apply the scaling method to the dataset used for modeling
X_scaled = ss.fit_transform(malData)
X_train_S, X_test_S, y_train_S, y_test_S = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42)

select.fit(X_train_S, y_train_S)
X_train_S = select.transform(X_train_S)
X_test_S = select.transform(X_test_S)

# Fit the logistic regression model to the training data.
logModel = clf.fit(X_train_S, y_train_S)
# Score the model on the test data
print('Training of Data successfully done after model building\n')

print('_________ Accuracy on the Training dataset __________')
pred = logModel.predict(X_train_S)
print(accuracy_score(y_train_S, pred))

print('\n_________ Classification Report Training set __________')
print(classification_report(y_train_S, pred))


print('\n_________ Accuracy on the Test dataset __________')
pred = logModel.predict(X_test_S)
print(accuracy_score(y_test_S, pred))

print('\n_________ Classification Report Test set __________')
print(classification_report(y_test_S, pred))

print('\n_________ f1_score __________')
print(f1_score(y_test_S, pred))

print('\n-------   Confusion Matrix for Logistic Regression  --------\n')

title1 = 'Confusion Matrix for Logistic Regression'
cm = confusion_matrix(y_test_S, pred, labels=logModel.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm,
                              display_labels=logModel.classes_)
print(disp.confusion_matrix)
print(title1)
disp.plot()
plt.title("Confusion matrix for Logistic Regression")
plt.show()

print('\n------------------------------------------------------------------\n')
#
#
#
#
#


print('\n--------------------------------Random Forest----------------------------------\n')

clf = RandomForestClassifier(
    n_estimators=110, random_state=0, criterion='entropy', min_samples_split=3, min_samples_leaf=1, max_depth=None, max_features=6)
randomModel = clf.fit(X_train, y_train)
print('Training of Data successfully done after model building\n')

print('_________ Accuracy on the Training dataset __________')
train_pred = randomModel.predict(X_train)
print(accuracy_score(y_train, train_pred))

print('\n_________ Classification Report training set __________')
print(classification_report(y_train, train_pred))

print('\n_________ Accuracy on the Test dataset __________')
pred = randomModel.predict(X_test)
print(accuracy_score(y_test, pred))

print('\n_________ Classification Report test set __________')
print(classification_report(y_test, pred))

print('\n_________ f1_score __________')
print(f1_score(y_test, pred))

print('\n-------   Confusion Matrix for Random Forest  --------\n')
title1 = 'Confusion Matrix'
cm = confusion_matrix(y_test, pred, labels=randomModel.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm,
                              display_labels=randomModel.classes_)
print(disp.confusion_matrix)
print(title1)
disp.plot()
plt.title("Confusion matrix for Random Forest")
plt.show()


print('\n-------------------------------------------- Support Vector Machine(SVM)=== standardized ----------------------------------------------\n')

clf = svm.SVC(random_state=0, kernel='rbf', C=100, gamma=1)
svmModel = clf.fit(X_train_S, y_train_S)

print('Training of Data successfully done after SVM model building\n')

print('_________ Accuracy on the Training dataset __________')
pred = svmModel.predict(X_train_S)
print(accuracy_score(y_train_S, pred))

print('\n_________ Classification Report Train set __________')
print(classification_report(y_train_S, pred))


print('\n_________ Accuracy on the Test dataset __________')
pred = svmModel.predict(X_test_S)
print(accuracy_score(y_test_S, pred))

print('\n_________ Classification Report Test set __________')
print(classification_report(y_test_S, pred))


print('\n_________ f1_score __________')
print(f1_score(y_test_S, pred))

print('\n-------   Confusion Matrix for Support Vector Machine --------\n')

title1 = 'Confusion Matrix for Support Vector Machine'
cm = confusion_matrix(y_test_S, pred, labels=svmModel.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm,
                              display_labels=svmModel.classes_)
print(disp.confusion_matrix)
print(title1)
disp.plot()
plt.title("Confusion matrix for SVM")
plt.show()
print('\n------------------------------------------------------------------\n')

#
#
#
#
#

print('\n-------------------------------- Gradient Boosting Classifier ----------------------------------\n')
gb_clf = GradientBoostingClassifier(
    n_estimators=20, learning_rate=0.75, max_features=2, max_depth=20, random_state=0)
gbModel = gb_clf.fit(X_train, y_train)

print('_________ Accuracy on the Training dataset __________')
train_gb = gbModel.predict(X_train)
print(accuracy_score(y_train, train_gb))

print('\n_________ Classification Report Train set __________')
print(classification_report(y_train, train_gb))


print('\n_________ Accuracy on the Test dataset __________')
pred = gbModel.predict(X_test)
print(accuracy_score(y_test, pred))

print('\n_________ Classification Report Test set __________')
print(classification_report(y_test, pred))


print('\n_________ f1_score __________')
print(f1_score(y_test, pred))

print('\n-------   Confusion Matrix for Gradient Booster  --------\n')

title1 = 'Confusion Matrix without Standardization'
cm = confusion_matrix(y_test, pred, labels=gbModel.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm,
                              display_labels=gbModel.classes_)
print(disp.confusion_matrix)
print(title1)
disp.plot()
plt.title("Confusion matrix for Gradient Booster")
plt.show()
#
#
#
#

print('\n-------------------------------- Gradient Boosting with XGBoost  ----------------------------------\n')
gb_clf = XGBClassifier(n_estimators=120, learning_rate=0.25,
                       max_depth=20, min_child_weight=1, colsample_bytree=0.7, random_state=42)
XGModel = gb_clf.fit(X_train, y_train)

print('_________ Accuracy on the Training dataset __________')
train_gb = XGModel.predict(X_train)
print(accuracy_score(y_train, train_gb))

print('\n_________ Classification Report Train set __________')
print(classification_report(y_train, train_gb))


print('\n_________ Accuracy on the Test dataset __________')
pred = XGModel.predict(X_test)
print(accuracy_score(y_test, pred))

print('\n_________ Classification Report Test set __________')
print(classification_report(y_test, pred))


print('\n_________ f1_score __________')
print(f1_score(y_test, pred))

print('\n-------   Confusion Matrix for XGBoost  --------\n')

title1 = 'Confusion Matrix For XGBoost'
cm = confusion_matrix(y_test, pred, labels=XGModel.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm,
                              display_labels=XGModel.classes_)
print(disp.confusion_matrix)
plt.show()
print(title1)
disp.plot()
plt.title("Confusion matrix for XGBoost")
plt.show()

#
#
#
#

print('\n-------------------------------- Gradient Boosting with lightGBM  ----------------------------------\n')
gb_clf = lgb.LGBMClassifier(n_estimators=110,
                            learning_rate=0.25, max_depth=20, num_leaves=35, random_state=42, max_bin=50)
lightGBMmodel = gb_clf.fit(X_train, y_train)

print('\n_________ Accuracy on the Training dataset __________')
train_gb = lightGBMmodel.predict(X_train)
print(accuracy_score(y_train, train_gb))

print('\n_________ Classification Report Train set __________')
print(classification_report(y_train, train_gb))

print('\n_________ Accuracy on the Test dataset __________')
pred = lightGBMmodel.predict(X_test)
print(accuracy_score(y_test, pred))

print('\n_________ Classification Report Test set __________')
print(classification_report(y_test, pred))


print('\n_________ f1_score __________')
print(f1_score(y_test, pred))

print('\n-------   Confusion Matrix for LiggthGBM  --------\n')

title1 = 'Confusion Matrix for lightGBM'
cm = confusion_matrix(y_test, pred, labels=lightGBMmodel.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm,
                              display_labels=lightGBMmodel.classes_)
print(disp.confusion_matrix)
print(title1)
disp.plot()
plt.title("Confusion matrix for lightGBMmodel")
plt.show()

#
#
#
#


print('\n--------------------------------Naive Bayes===standardized----------------------------------\n')
nb = GaussianNB()
nbModel = nb.fit(X_train_S, y_train_S)

print('Training of Data successfully done after model building\n')

print('_________ Accuracy on the Training dataset __________')
pred = nbModel.predict(X_train_S)
print(accuracy_score(y_train_S, pred))

print('\n_________ Classification Report training set __________')
print(classification_report(y_train_S, pred))

print('\n_________ Accuracy on the Test dataset __________')
pred = nbModel.predict(X_test_S)
print(accuracy_score(y_test_S, pred))

print('\n_________ Classification Report test set __________')
print(classification_report(y_test_S, pred))

print('\n_________ F1_Score __________')
print(f1_score(y_test_S, pred))


print('\n-------   Confusion Matrix for Naive Bayes  --------\n')

title1 = 'Confusion Matrix for Naive Bayes'
cm = confusion_matrix(y_test_S, pred, labels=nbModel.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm,
                              display_labels=nbModel.classes_)
print(disp.confusion_matrix)
print(title1)
disp.plot()
plt.title("Confusion matrix for Naive Bayes")
plt.show()


#
#
#
#
#

print('\n-------------------------------KNN classifier==standardized----------------------------------\n')
knn = KNeighborsClassifier(
    n_neighbors=4, metric='euclidean')
knnModel = knn.fit(X_train_S, y_train_S)

print('Training of Data successfully done after model building\n')

print('_________ Accuracy on the Training dataset __________')
pred = knnModel.predict(X_train_S)
print(accuracy_score(y_train_S, pred))

print('\n_________ Classification Report training set __________')
print(classification_report(y_train_S, pred))

print('\n_________ Accuracy on the Test dataset __________')
pred = knnModel.predict(X_test_S)
print(accuracy_score(y_test_S, pred))

print('\n_________ Classification Report test set __________')
print(classification_report(y_test_S, pred))

print('\n_________ F1_Score __________')
print(f1_score(y_test_S, pred))


print('\n-------   Confusion Matrix for kNN classifier --------\n')

title1 = 'Confusion Matrix without Standardization'
cm = confusion_matrix(y_test_S, pred, labels=knnModel.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm,
                              display_labels=knnModel.classes_)
print(disp.confusion_matrix)

print(title1)
disp.plot()
plt.title("Confusion matrix for KNN classifier")
plt.show()


# ROC and AUC curves for different classifiers

y_pred_logistic = logModel.decision_function(X_test_S)
# probabilities for the positive outcome is kept
y_pred_randomModel = randomModel.predict_proba(X_test)[:, 1]
y_pred_svm = svmModel.decision_function(X_test_S)
y_pred_GB = gbModel.predict_proba(X_test)[:, 1]
y_pred_XG = XGModel.predict_proba(X_test)[:, 1]
y_pred_lgbm = lightGBMmodel.predict_proba(X_test)[:, 1]
y_pred_nb = nbModel.predict_proba(X_test_S)[:, 1]
y_pred_knn = knnModel.predict_proba(X_test_S)[:, 1]

logistic_fpr, logistic_tpr, threshold = roc_curve(y_test_S, y_pred_logistic)
auc_logistic = auc(logistic_fpr, logistic_tpr)

svm_fpr, svm_tpr, threshold = roc_curve(y_test_S, y_pred_svm)
auc_svm = auc(svm_fpr, svm_tpr)

random_fpr, random_tpr, threshold = roc_curve(y_test, y_pred_randomModel)
auc_random = auc(random_fpr, random_tpr)

gb_fpr, gb_tpr, threshold = roc_curve(y_test, y_pred_GB)
auc_gb = auc(gb_fpr, gb_tpr)

xg_fpr, xg_tpr, threshold = roc_curve(y_test, y_pred_XG)
auc_xg = auc(xg_fpr, xg_tpr)

lgbm_fpr, lgbm_tpr, threshold = roc_curve(y_test, y_pred_lgbm)
auc_lgbm = auc(lgbm_fpr, lgbm_tpr)

nb_fpr, nb_tpr, threshold = roc_curve(y_test_S, y_pred_nb)
auc_nb = auc(nb_fpr, nb_tpr)

knn_fpr, knn_tpr, threshold = roc_curve(y_test_S, y_pred_knn)
auc_knn = auc(knn_fpr, knn_tpr)

plt.figure(figsize=(5, 5), dpi=100)
plt.plot(logistic_fpr, logistic_tpr, marker='.',
         label='Logistic (auc = %0.3f)' % auc_logistic)
plt.plot(svm_fpr, svm_tpr, linestyle='-', label='SVM (auc = %0.3f)' % auc_svm)
plt.plot(random_fpr, random_tpr, marker='1',
         label='Random (auc = %0.3f)' % auc_random)
plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Random guess')

plt.plot(gb_fpr, gb_tpr, linestyle='--', label='GB (auc = %0.3f)' % auc_gb)
plt.plot(xg_fpr, xg_tpr, linestyle='solid', label='XG (auc = %0.3f)' % auc_xg)
plt.plot(lgbm_fpr, lgbm_tpr, linestyle='dashdot',
         label='LGBM (auc = %0.3f)' % auc_lgbm)
plt.plot(nb_fpr, nb_tpr, linestyle=':',
         label='NB (auc = %0.3f)' % auc_nb)
plt.plot(knn_fpr, knn_tpr, linestyle=':',
         label='KNN (auc = %0.3f)' % auc_knn)

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')

plt.legend()

plt.show()
