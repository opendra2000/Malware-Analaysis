from xgboost import plot_importance
import sklearn.feature_selection
import matplotlib.pyplot as plt
from sklearn import svm
import numpy as np
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics import roc_curve, roc_auc_score
from xgboost import XGBClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import f1_score, accuracy_score, plot_confusion_matrix, auc, confusion_matrix, ConfusionMatrixDisplay, classification_report
import seaborn as sns
import lightgbm as lgb
from sklearn.feature_selection import f_classif
from sklearn.feature_selection import SelectKBest
from matplotlib import pyplot


malData = pd.read_csv("data/finaldata.csv", sep=",", low_memory=True)
print('\n--------------------------------------------Shape of Dataset--------------------------------------------\n')
print(malData.shape)
print('\n Dataset keys')
print(malData.keys())

print('\n--------------------------------------------First five rows: dataset.head()--------------------------------------------\n')
print(malData.head())
print('\n--------------------------------------------DataFrame info- dataset.info()--------------------------------------------\n')
print(malData.info())

print('\n--------------------------------------------Data Cleaning--------------------------------------------\n')
y = malData['label']  # this is the target variable
malData = malData.drop(['hash', 'label'], axis=1)
print(malData.head())
print('Hash and label removed successfully\n')

print('\n--------------------------------------------Data types of features--------------------------------------------\n')
print(malData.dtypes)


print('\n--------------------------------------------Dataset Description for columns: dataset.head()--------------------------------------------\n')
print(malData.describe())


print('\n--------------------------------------------Splitting the dataset--------------------------------------------\n')
X_train, X_test, y_train, y_test = train_test_split(
    malData, y, test_size=0.2, random_state=42)

X_train_backup = X_train
print('X_train.shape = ' + str(X_train.shape))
print('X_test.shape = ' + str(X_test.shape))
print('y_train.shape = ' + str(y_train.shape))
print('y_test.shape = ' + str(y_test.shape))

# using feature selection
select = SelectKBest(score_func=f_classif, k=20)
selected_features = select.fit(X_train, y_train)

# transform train input data
X_train = select.transform(X_train)
# transform test input data
X_test = select.transform(X_test)

featuress1 = list(X_train_backup.columns)

featuress = pd.array(X_train_backup.columns)
filter = select.get_support()
print(featuress[filter])

for i in range(len(select.scores_)):
    print('%s: %f' % (featuress1[i], select.scores_[i]))

print('\n-------------------------------------------- HyperParameter Tuning and best model selection--------------------------------------------\n')

model_params = {
    'svm': {
        'model': svm.SVC(gamma='auto'),
        'params': {
            "kernel": ["rbf"],
            "C": [0.1, 1, 10, 100],
            'gamma': [1, 0.1, 0.01, 0.001]
        }
    },
    'random_forest': {
        'model': RandomForestClassifier(),
        'params': {
            'n_estimators': [100, 110, 120],
            'criterion': ['entropy', 'gini'],
            'max_depth': [None, 1, 3, 5, 7, 9],
            'max_features': range(1, 11),
            'min_samples_split': range(2, 10),
            'min_samples_leaf': [1, 3, 5]
        }
    },
    'logistic_regression': {
        'model': LogisticRegression(),
        'params': {
            'C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000],
            'penalty': ['l1', 'l2', 'elasticnet'],
            'class_weight': [{1: 0.5, 0: 0.5}, {1: 0.4, 0: 0.6}, {1: 0.6, 0: 0.4}, {1: 0.7, 0: 0.3}],
            'max_iter': [100, 1000, 5000]
        }
    },
    'gradient_boosting': {
        'model': GradientBoostingClassifier(),
        'params': {
            "n_estimators": [5, 20, 250, 500],
            "max_depth": [1, 3, 5, 7, 9],
            "learning_rate": [0.01, 0.1, 0.75, 1, 10]
        }
    },
    'light_gbm': {
        'model': lgb.LGBMClassifier(),
        'params': {
            'n_estimators': [100, 110, 120],
            'learning_rate': [0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1],
            'max_depth': [5, 7, 9, 20],
            'num_leaves': [20, 35, 100],
            'max_bin': [30, 50, 60]
        }
    },
    'naive_bayes_gaussian': {
        'model': GaussianNB(),
        'params': {
            "gamma": [0.1, 1.0, 10, 100],
            "C": [0.1, 1.0, 10, 100]
        }
    }
}

scores = []

# hyperparameter tuning and best model selection using GridSearchCV
for model_name, mp in model_params.items():
    clf = GridSearchCV(mp['model'], mp['params'],
                       cv=10, verbose=2, n_jobs=-1, return_train_score=False)  # cross validation of 5
    print('gridsearch cv done')
    clf.fit(X_train, y_train)  # model training
    print('Training done')
    scores.append({
        'model': model_name,
        'best_score': clf.best_score_,
        'best_params': clf.best_params_
    })

df = pd.DataFrame(scores, columns=['model', 'best_score', 'best_params'])
pd.set_option('display.max_colwidth', None)
print(df)
